import re
import requests
from bs4 import BeautifulSoup
from typing import List, Tuple, Optional
import streamlit as st
import time
import json
import urllib.parse
import random

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì „ì²˜ë¦¬"""
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ì–´, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€)
    text = re.sub(r'[^\w\sê°€-í£.,!?;:()\-]', '', text)
    
    return text.strip()

def get_robust_headers() -> dict:
    """ë” ê°•ë ¥í•œ í—¤ë” ì„¤ì •"""
    try:
        from fake_useragent import UserAgent
        ua = UserAgent()
        user_agent = ua.random
    except:
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    
    return {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'Referer': 'https://www.google.com/'
    }

def try_cloudscraper(url: str) -> Optional[Tuple[str, List[str]]]:
    """Cloudflare ë³´í˜¸ ì‚¬ì´íŠ¸ë¥¼ ìœ„í•œ cloudscraper ì‹œë„"""
    try:
        import cloudscraper
        
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'desktop': True
            }
        )
        
        response = scraper.get(url, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'noscript', 'iframe']):
                tag.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_parts = []
            headings = []
            
            # ì œëª© ì¶”ì¶œ
            title = soup.find('title')
            if title:
                title_text = title.get_text().strip()
                if title_text:
                    text_parts.append(f"ì œëª©: {title_text}")
            
            # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                text_parts.append(f"ì„¤ëª…: {meta_desc['content']}")
            
            # ì£¼ìš” í—¤ë”© ì¶”ì¶œ
            for heading in soup.find_all(['h1', 'h2', 'h3', 'h4']):
                heading_text = heading.get_text().strip()
                if heading_text and len(heading_text) > 3:
                    headings.append(heading_text)
                    text_parts.append(heading_text)
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_selectors = [
                'main', 'article', '.content', '.main-content', '.post-content',
                '.job-description', '.job-content', '.description', '.details',
                '[role="main"]', '.container', '.wrapper', '.job-detail',
                '.recruit-content', '.job-info', '.position-detail', '.job-text'
            ]
            
            main_content = None
            for selector in content_selectors:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if main_content:
                content_text = main_content.get_text()
                text_parts.append(content_text)
            else:
                body = soup.find('body')
                if body:
                    content_text = body.get_text()
                    text_parts.append(content_text)
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
            full_text = '\n'.join(text_parts)
            cleaned_text = clean_text(full_text)
            
            return cleaned_text, headings
            
    except ImportError:
        return None
    except Exception as e:
        return None

def extract_text_with_selenium(url: str) -> Tuple[str, List[str]]:
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ ë™ì  ì½˜í…ì¸ ê°€ ìˆëŠ” ì›¹ì‚¬ì´íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.service import Service
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from webdriver_manager.chrome import ChromeDriverManager
        
        # Chrome ì˜µì…˜ ì„¤ì •
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument(f"--user-agent={get_robust_headers()['User-Agent']}")
        
        # ì¶”ê°€ ì•ˆí‹°ë´‡ ê°ì§€ ë°©ì§€ ì˜µì…˜
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")
        chrome_options.add_argument("--disable-javascript")  # ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œëŠ” JS ë¹„í™œì„±í™”ê°€ ë„ì›€ë  ìˆ˜ ìˆìŒ
        
        # WebDriver ì„¤ì •
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
        driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko', 'en-US', 'en']})")
        
        try:
            # í˜ì´ì§€ ë¡œë“œ
            driver.get(url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ìµœëŒ€ 15ì´ˆ)
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ì¶”ê°€ ëŒ€ê¸° (JavaScript ë¡œë”©ì„ ìœ„í•´)
            time.sleep(5)
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (ë™ì  ì½˜í…ì¸  ë¡œë”©ì„ ìœ„í•´)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = driver.page_source
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'noscript', 'iframe']):
                tag.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_parts = []
            
            # ì œëª© ì¶”ì¶œ
            title = soup.find('title')
            if title:
                title_text = title.get_text().strip()
                if title_text:
                    text_parts.append(f"ì œëª©: {title_text}")
            
            # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                text_parts.append(f"ì„¤ëª…: {meta_desc['content']}")
            
            # ì£¼ìš” í—¤ë”© ì¶”ì¶œ
            headings = []
            for heading in soup.find_all(['h1', 'h2', 'h3', 'h4']):
                heading_text = heading.get_text().strip()
                if heading_text and len(heading_text) > 3:
                    headings.append(heading_text)
                    text_parts.append(heading_text)
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_selectors = [
                'main', 'article', '.content', '.main-content', '.post-content',
                '.job-description', '.job-content', '.description', '.details',
                '[role="main"]', '.container', '.wrapper', '.job-detail',
                '.recruit-content', '.job-info', '.position-detail'
            ]
            
            main_content = None
            for selector in content_selectors:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if main_content:
                content_text = main_content.get_text()
                text_parts.append(content_text)
            else:
                body = soup.find('body')
                if body:
                    content_text = body.get_text()
                    text_parts.append(content_text)
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
            full_text = '\n'.join(text_parts)
            cleaned_text = clean_text(full_text)
            
            return cleaned_text, headings
            
        finally:
            driver.quit()
            
    except ImportError:
        st.warning("âš ï¸ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install selenium webdriver-managerë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        return "", []
    except Exception as e:
        st.error(f"Seleniumì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return "", []

def try_api_endpoint(url: str) -> Optional[str]:
    """API ì—”ë“œí¬ì¸íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë°ì´í„° ì¶”ì¶œ ì‹œë„"""
    try:
        # URLì—ì„œ API ì—”ë“œí¬ì¸íŠ¸ íŒ¨í„´ ì°¾ê¸°
        parsed_url = urllib.parse.urlparse(url)
        path_parts = parsed_url.path.split('/')
        
        # ì¼ë°˜ì ì¸ API íŒ¨í„´ë“¤
        api_patterns = [
            f"{parsed_url.scheme}://{parsed_url.netloc}/api/jobs/{path_parts[-1]}",
            f"{parsed_url.scheme}://{parsed_url.netloc}/api/recruit/{path_parts[-1]}",
            f"{parsed_url.scheme}://{parsed_url.netloc}/api/positions/{path_parts[-1]}",
        ]
        
        headers = get_robust_headers()
        headers['Accept'] = 'application/json'
        
        for api_url in api_patterns:
            try:
                response = requests.get(api_url, headers=headers, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    # JSONì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text_parts = []
                    if 'title' in data:
                        text_parts.append(f"ì œëª©: {data['title']}")
                    if 'description' in data:
                        text_parts.append(data['description'])
                    if 'requirements' in data:
                        text_parts.append(f"ìš”êµ¬ì‚¬í•­: {data['requirements']}")
                    if 'content' in data:
                        text_parts.append(data['content'])
                    
                    if text_parts:
                        return '\n'.join(text_parts)
            except:
                continue
        
        return None
    except:
        return None

def extract_text_from_url(url: str) -> Tuple[str, List[str]]:
    """URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë‹¤ì¤‘ ì „ëµ)"""
    
    with st.spinner("ğŸ” ì›¹í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
        # 1ë‹¨ê³„: API ì—”ë“œí¬ì¸íŠ¸ ì‹œë„
        api_text = try_api_endpoint(url)
        if api_text and len(api_text) > 100:
            return clean_text(api_text), []
        
        # 2ë‹¨ê³„: Cloudscraper ì‹œë„ (Cloudflare ë³´í˜¸ ì‚¬ì´íŠ¸ìš©)
        cloudscraper_result = try_cloudscraper(url)
        if cloudscraper_result and len(cloudscraper_result[0]) > 200:
            return cloudscraper_result
        
        # 3ë‹¨ê³„: ê¸°ë³¸ HTTP ìš”ì²­
    try:
        headers = get_robust_headers()
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        # ì‘ë‹µ ì¸ì½”ë”© í™•ì¸ ë° ì„¤ì •
        if response.encoding == 'ISO-8859-1':
            response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'noscript', 'iframe']):
            tag.decompose()
        
        # íŠ¹ì • í´ë˜ìŠ¤ë‚˜ IDë¥¼ ê°€ì§„ ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
        unwanted_selectors = [
            '.advertisement', '.ads', '.banner', '.sidebar', '.navigation',
            '.menu', '.footer', '.header', '.cookie-notice', '.popup',
            '.modal', '.overlay', '.loading', '.spinner', '.breadcrumb'
        ]
        
        for selector in unwanted_selectors:
            for element in soup.select(selector):
                element.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_parts = []
        
        # ì œëª© ì¶”ì¶œ
        title = soup.find('title')
        if title:
            title_text = title.get_text().strip()
            if title_text:
                text_parts.append(f"ì œëª©: {title_text}")
        
        # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            text_parts.append(f"ì„¤ëª…: {meta_desc['content']}")
        
        # ì£¼ìš” í—¤ë”© ì¶”ì¶œ
        headings = []
        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4']):
            heading_text = heading.get_text().strip()
            if heading_text and len(heading_text) > 3:
                headings.append(heading_text)
                text_parts.append(heading_text)
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë” ì •êµí•˜ê²Œ)
        content_selectors = [
            'main', 'article', '.content', '.main-content', '.post-content',
            '.job-description', '.job-content', '.description', '.details',
            '[role="main"]', '.container', '.wrapper', '.job-detail',
            '.recruit-content', '.job-info', '.position-detail', '.job-text'
        ]
        
        main_content = None
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        if main_content:
            content_text = main_content.get_text()
            text_parts.append(content_text)
        else:
            body = soup.find('body')
            if body:
                content_text = body.get_text()
                text_parts.append(content_text)
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
        full_text = '\n'.join(text_parts)
        cleaned_text = clean_text(full_text)
        
        # 4ë‹¨ê³„: í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ Selenium ì‹œë„
        if len(cleaned_text) < 200:
            selenium_text, selenium_headings = extract_text_with_selenium(url)
            
            if selenium_text and len(selenium_text) > len(cleaned_text):
                cleaned_text = selenium_text
                headings = selenium_headings
        
        return cleaned_text, headings
        
    except requests.exceptions.Timeout:
        return extract_text_with_selenium(url)
    except requests.exceptions.ConnectionError:
        return "", []
    except requests.exceptions.HTTPError as e:
        return "", []
    except Exception as e:
        return "", []

def extract_all_text(job_text: str) -> str:
    """ì…ë ¥ë°›ì€ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ì—¬ ë°˜í™˜ (MVP ë‹¨ê³„)"""
    return clean_text(job_text)

# ê¸°ì¡´ URL í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ì€ ì£¼ì„ ì²˜ë¦¬ (í–¥í›„ í™•ì¥ìš©)
"""
def extract_text_from_image(image_url: str) -> str:
    # ì´ë¯¸ì§€ OCR ê¸°ëŠ¥ (í–¥í›„ í™•ì¥ìš©)
    pass
""" 